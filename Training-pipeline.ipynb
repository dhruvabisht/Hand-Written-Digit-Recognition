{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91311e2c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6a2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649abec5",
   "metadata": {},
   "source": [
    "# Load MNIST Handwritten Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e71f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets \n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1093d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augs = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.ToTensor(),#convert image to tensor, it gives a new shape to the image, (chanel, height, width)\n",
    "    T.Normalize(mean = 0.5, std=0.5)\n",
    "])\n",
    "\n",
    "\n",
    "valid_augs = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=0.5, std=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93bf2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the training and the testing datasets\n",
    "trainset = datasets.MNIST('./',download=True, train = True, transform = train_augs)\n",
    "testset = datasets.MNIST('./',download=True, train = False, transform = valid_augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0622d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting into train and validation\n",
    "trainset, validset = torch.utils.data.random_split(trainset,[50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f36e146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainset : 50000\n",
      "Size of validset : 10000\n",
      "Size of testset : 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of trainset : {len(trainset)}\")\n",
    "print(f\"Size of validset : {len(validset)}\")\n",
    "print(f\"Size of testset : {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c014f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of image : torch.Size([1, 28, 28])\n",
      "For visualization we need (h x w x c) so using permute shape will be : torch.Size([28, 28, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOe0lEQVR4nO3dbaxVZXrG8esqOjSDSlAonqApU6VJRx2dii9JsZ0GfIEvIhIdEicYtWc+aNXE1hIaIyYmEtPRjBqwZwIR26k6yWA0zaQDQ5vQSVMjmlMFlZESRQhwnNB08IuC3P1wlvaMnv3s435bm3P/f8nJ3nvde+11Z8nlWnu97McRIQCT3+/U3QCA3iDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO8Zl+49s/6vt/7W9x/YNdfeE9hB2fIntUyS9JOmfJZ0paVDSP9r+w1obQ1vMFXT4ItsXSvpPSadH9Q/E9hZJr0TEA7U2h5axZcdEWdKFdTeB1hF2jGe3pBFJf237VNvXSPozSV+vty20g914jMv2tyQ9qdGt+Q5JH0r6OCJur7UxtIywY0Js/4ekTRHx93X3gtawG49x2f6W7d+1/XXbfyVpQNIzNbeFNhB2NPI9SQc1+t19oaSrI+LjeltCO9iNB5Jgyw4kQdiBJAg7kARhB5I4pZcLs83RQKDLIsLjTW9ry277Otu7q1sgV7XzWQC6q+VTb7anSPqVpKsl7Zf0qqQVEfFWYR627ECXdWPLfrmkPRGxNyI+kfS8pOvb+DwAXdRO2OdI+mDM6/3VtN9ie9D2Dts72lgWgDZ1/QBdRAxJGpLYjQfq1M6W/YCkc8e8PqeaBqAPtRP2VyXNs/0N21+T9F1JL3emLQCd1vJufEQct32XpJ9LmiJpY0Ts6lhnADqqp3e98Z0d6L6uXFQD4ORB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii5fHZJcn2e5KOSvpU0vGImN+JpgB0Xlthr/x5RPy6A58DoIvYjQeSaDfsIWmL7ddsD473BtuDtnfY3tHmsgC0wRHR+sz2nIg4YPv3JG2V9JcRsb3w/tYXBmBCIsLjTW9ryx4RB6rHEUkvSrq8nc8D0D0th932NNunf/Zc0jWSdnaqMQCd1c7R+NmSXrT92ef8U0T8S0e6AtBxbX1n/8oL4zs70HVd+c4O4ORB2IEkCDuQBGEHkiDsQBKduBEGfWzWrFnF+oIFC4r1ZcuWFetXXHFFsT5v3rxivWT//v3F+po1a4r1DRs2tLzsyYgtO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwV1vk8CiRYsa1h555JHivJdeemmxfujQoWL92LFjxXrJaaedVqzPmDGjrWWvX7++Ye3ee+8tznsy4643IDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC8+wngcWLFxfrL7zwQsNas3PZpXPRkrRq1api/ejRo8V6yTnnnFOsN7tf/bbbbivWT5w40bB27bXXFufdtm1bsd7POM8OJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0nwu/F9YObMmcX62rVri/WRkZGGteXLlxfn3bp1a7Hezeswmv0u/EMPPVSsX3zxxcV66V79s88+uzjvZNR0y257o+0R2zvHTDvT9lbb71aP5V8ZAFC7iezGPyPpui9MWyVpW0TMk7Steg2gjzUNe0Rsl3TkC5Ovl7Sper5J0tLOtgWg01r9zj47Ig5Wzw9Jmt3ojbYHJQ22uBwAHdL2AbqIiNINLhExJGlI4kYYoE6tnno7bHtAkqrHxoeDAfSFVsP+sqSV1fOVkl7qTDsAuqXpbrzt5yR9R9JM2/slPShpraSf2L5d0vuSbupmk5PdU089VaxfdNFFxfqtt97asLZly5ZWWuoJe9zbrj938803F+vNzrN/9NFHDWu7du0qzjsZNQ17RKxoUFrY4V4AdBGXywJJEHYgCcIOJEHYgSQIO5AEt7j2gTPOOKNY3717d7G+efPmTrbTUdOnT29YW7duXXHeFSsanQiamNJw1cPDw2199smILTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMGQzT3Q7GeLP/jgg2K92a2gDzzwQMNasyGZS+fBJWnWrFnF+sKF5ZsfS0M+N1t2M4899lixfv/99zeslYZzPtkxZDOQHGEHkiDsQBKEHUiCsANJEHYgCcIOJMF59h5odr/6hg0bivVly5YV66Xz8Pv27SvOO23atGL9rLPOKtbb0ezf3j333FOsP/3008X68ePHv3JPkwHn2YHkCDuQBGEHkiDsQBKEHUiCsANJEHYgCc6znwQWL15crJfuGb/qqqs63U7HNBuq+u677+5RJ5NLy+fZbW+0PWJ755hpa2wfsD1c/S3pZLMAOm8iu/HPSLpunOmPR8Ql1d/POtsWgE5rGvaI2C7pSA96AdBF7Rygu8v2G9Vu/oxGb7I9aHuH7R1tLAtAm1oN+3pJ50m6RNJBST9o9MaIGIqI+RExv8VlAeiAlsIeEYcj4tOIOCHpR5Iu72xbADqtpbDbHhjz8gZJOxu9F0B/aHqe3fZzkr4jaaakw5IerF5fIikkvSfp+xFxsOnCOM/eFVOnTm1YW7KkfFb0iSeeKNbnzJnTUk+f2bVrV8PaokWLivMePny4rWVn1eg8+ykTmHHFOJPLv7YAoO9wuSyQBGEHkiDsQBKEHUiCsANJcItrcnv37i3W586dW6wfO3asWL/gggsa1vbs2VOcF63hp6SB5Ag7kARhB5Ig7EAShB1IgrADSRB2IImmd73h5DYwMFCsz5o1q63PHxoaKtY5l94/2LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKcZ58ETjml8X/G1atXF+edNm1asb5v375i/b777ivW0T/YsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEk3Ps9s+V9KzkmZrdIjmoYj4oe0zJb0gaa5Gh22+KSL+p3utopFbbrmlYe3OO+8szvvhhx8W60uXLi3WP/nkk2Id/WMiW/bjku6LiG9KulLSnba/KWmVpG0RMU/Stuo1gD7VNOwRcTAiXq+eH5X0tqQ5kq6XtKl62yZJS7vUI4AO+Erf2W3PlfRtSa9Imh0RB6vSIY3u5gPoUxO+Nt72aZJ+KuneiPiN/f/DSUVENBrHzfagpMF2GwXQnglt2W2fqtGg/zgiNleTD9seqOoDkkbGmzcihiJifkTM70TDAFrTNOwe3YRvkPR2RDw2pvSypJXV85WSXup8ewA6ZSK78X8i6XuS3rQ9XE1bLWmtpJ/Yvl3S+5Ju6kqHaOrGG29sed5HH320WB8eHm75s9FfmoY9In4padzxniUt7Gw7ALqFK+iAJAg7kARhB5Ig7EAShB1IgrADSThi3Ktcu7OwBpfUouyOO+4o1tetW9ew9s477xTnveyyy4r1jz/+uFhH/4mIcU+Vs2UHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYsrkPnH/++cX6gw8+WKxPmTKlYe3xxx8vzst59DzYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEtzP3gNTp04t1rdv316sN7vnfPPmzQ1ry5cvL86LyYf72YHkCDuQBGEHkiDsQBKEHUiCsANJEHYgiab3s9s+V9KzkmZLCklDEfFD22sk/YWkD6u3ro6In3Wr0X525ZVXFusbN24s1qdPn16s7927t1h//vnni3VAmtiPVxyXdF9EvG77dEmv2d5a1R6PiL/rXnsAOqVp2CPioKSD1fOjtt+WNKfbjQHorK/0nd32XEnflvRKNeku22/Y3mh7RoN5Bm3vsL2jvVYBtGPCYbd9mqSfSro3In4jab2k8yRdotEt/w/Gmy8ihiJifkTMb79dAK2aUNhtn6rRoP84IjZLUkQcjohPI+KEpB9Jurx7bQJoV9Ow27akDZLejojHxkwfGPO2GyTt7Hx7ADql6S2uthdI+ndJb0o6UU1eLWmFRnfhQ9J7kr5fHcwrfdZJe4vreeed17DWbFjkhx9+uFh/8skni/UjR44U68BYjW5xncjR+F9KGm/mlOfUgZMVV9ABSRB2IAnCDiRB2IEkCDuQBGEHkuCnpIFJhp+SBpIj7EAShB1IgrADSRB2IAnCDiRB2IEkJvLrsp30a0nvj3k9s5rWj/q1t37tS6K3VnWyt99vVOjpRTVfWri9o19/m65fe+vXviR6a1WvemM3HkiCsANJ1B32oZqXX9KvvfVrXxK9taonvdX6nR1A79S9ZQfQI4QdSKKWsNu+zvZu23tsr6qjh0Zsv2f7TdvDdY9PV42hN2J755hpZ9reavvd6nHcMfZq6m2N7QPVuhu2vaSm3s61/W+237K9y/Y91fRa112hr56st55/Z7c9RdKvJF0tab+kVyWtiIi3etpIA7bfkzQ/Imq/AMP2n0r6SNKzEXFhNe1RSUciYm31P8oZEfE3fdLbGkkf1T2MdzVa0cDYYcYlLZV0q2pcd4W+blIP1lsdW/bLJe2JiL0R8Ymk5yVdX0MffS8itkv64nAw10vaVD3fpNF/LD3XoLe+EBEHI+L16vlRSZ8NM17ruiv01RN1hH2OpA/GvN6v/hrvPSRtsf2a7cG6mxnH7DHDbB2SNLvOZsbRdBjvXvrCMON9s+5aGf68XRyg+7IFEfHHkhZLurPaXe1LMfodrJ/OnU5oGO9eGWeY8c/Vue5aHf68XXWE/YCkc8e8Pqea1hci4kD1OCLpRfXfUNSHPxtBt3ocqbmfz/XTMN7jDTOuPlh3dQ5/XkfYX5U0z/Y3bH9N0nclvVxDH19ie1p14ES2p0m6Rv03FPXLklZWz1dKeqnGXn5Lvwzj3WiYcdW87mof/jwiev4naYlGj8j/t6S/raOHBn39gaT/qv521d2bpOc0ult3TKPHNm6XdJakbZLelfQLSWf2UW//oNGhvd/QaLAGauptgUZ30d+QNFz9Lal73RX66sl643JZIAkO0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEv8HfZiK9YyDoYoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing an example image from the train set\n",
    "idx = 45\n",
    "image, label = trainset[idx]\n",
    "\n",
    "print(f'shape of image : {image.shape}')\n",
    "\n",
    "print(f'For visualization we need (h x w x c) so using permute shape will be : {image.permute(1, 2, 0).shape}')\n",
    "\n",
    "plt.imshow(image.permute(1, 2, 0), cmap = 'gray')\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335aba4",
   "metadata": {},
   "source": [
    "# Load Dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7c7cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c936f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64 #batch size set to 64\n",
    "trainloader = DataLoader(trainset, batch_size = bs, shuffle = True)\n",
    "validloader = DataLoader(validset, batch_size = bs)\n",
    "testloader = DataLoader(testset, batch_size = bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a48cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of batches in trainloader : 782\n",
      "Total no. of batches in validloader : 157\n",
      "Total no. of batches in testloader : 157\n"
     ]
    }
   ],
   "source": [
    "print(f'Total no. of batches in trainloader : {len(trainloader)}')\n",
    "print(f'Total no. of batches in validloader : {len(validloader)}')\n",
    "print(f'Total no. of batches in testloader : {len(testloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d1c00d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One image batch shape: torch.Size([64, 1, 28, 28])\n",
      "One labesl batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#printing the shape for labels and images in the batch\n",
    "for images, labels in trainloader:\n",
    "    print(f\"One image batch shape: {images.shape}\")\n",
    "    print(f\"One labesl batch shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca45152",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b155bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DigitModel(\n",
       "  (cnn_block): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_block): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1568, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import DigitModel\n",
    "\n",
    "model = DigitModel()\n",
    "model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c3b7f",
   "metadata": {},
   "source": [
    "# Create Train and Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f2c650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    #Using tqdm to get the progression\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        #Using 'cpu' becuase of cpu based environment, use cuda for a gpu device\n",
    "        images = images.to('cpu')\n",
    "        labels = labels.to('cpu')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        #getting the gradients and updating the parameters from the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss+=loss.item()\n",
    "        total_acc +=utils.multiclass_accuracy(logits, labels)\n",
    "        \n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afae506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #Using tqdm to get the progression\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            #Using 'cpu' becuase of cpu based environment, use cuda for a gpu device\n",
    "            images = images.to('cpu')\n",
    "            labels = labels.to('cpu')\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss+=loss.item()\n",
    "            total_acc +=utils.multiclass_accuracy(logits, labels)\n",
    "\n",
    "        return total_loss / len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7d43b",
   "metadata": {},
   "source": [
    "# Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9bc2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#using adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ab29b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f261e890574fe6865a6cbc7b62110a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed44281a9954a8dbb0d10b2adcf64d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 0.7577968581443857 Train Accuracy: 0.7397897839546204\n",
      "Epoch 1 Validation Loss: 0.28767065535230407 Validation Accuracy: 0.9104459881782532\n",
      "SAVED_BEST_MODEL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e76ef0d717484185da062baa2932f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba2fc2cd32640a7a45267695163ddc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 0.36763879430034885 Train Accuracy: 0.8868885636329651\n",
      "Epoch 2 Validation Loss: 0.1764595169488274 Validation Accuracy: 0.9462515711784363\n",
      "SAVED_BEST_MODEL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344cc844d88049f3b9da236db763a664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990e663bb73442809ddbb02dbd78d39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 0.28381006467773023 Train Accuracy: 0.9136428833007812\n",
      "Epoch 3 Validation Loss: 0.16875533677656632 Validation Accuracy: 0.9499480724334717\n",
      "SAVED_BEST_MODEL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516f427a3e134f60806bebf9a2fd14de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4eef55f339948569d1beedb48f38308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 0.24167479647090062 Train Accuracy: 0.9273697137832642\n",
      "Epoch 4 Validation Loss: 0.12996961379333224 Validation Accuracy: 0.9592191576957703\n",
      "SAVED_BEST_MODEL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b7be3f620e4190a4ba1334ba85a2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8aa2dd9081049d192e92045125c6a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss: 0.22542284198029114 Train Accuracy: 0.9322450160980225\n",
      "Epoch 5 Validation Loss: 0.11350884017489297 Validation Accuracy: 0.966232419013977\n",
      "SAVED_BEST_MODEL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a027bdb9474b6cb234629dec18b3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8804db1237584161919ef7691b9e32d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss: 0.20951119243212596 Train Accuracy: 0.9366008639335632\n",
      "Epoch 6 Validation Loss: 0.10291882765019679 Validation Accuracy: 0.9693294167518616\n",
      "SAVED_BEST_MODEL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a973434123a04fb58c4a5ad6a4304b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d57d29f61634218af158c6b1a4a0087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss: 0.20253359438503718 Train Accuracy: 0.9388986825942993\n",
      "Epoch 7 Validation Loss: 0.10421103717821181 Validation Accuracy: 0.9688299298286438\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = np.Inf\n",
    "\n",
    "#Taking an Epoch as 7 \n",
    "for i in range(7):\n",
    "    train_loss, train_acc = train_fn(model, trainloader, criterion, optimizer)\n",
    "    valid_loss, valid_acc = eval_fn(model, trainloader, criterion)\n",
    "    print(f\"Epoch {i+1} Train Loss: {train_loss} Train Accuracy: {train_acc}\")\n",
    "    print(f\"Epoch {i+1} Validation Loss: {valid_loss} Validation Accuracy: {valid_acc}\")\n",
    "    \n",
    "    #saving the best model state based on valid_loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(),\"best_weights.pt\")\n",
    "        print(\"SAVED_BEST_MODEL\")\n",
    "        best_valid_loss = valid_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfacb4ce",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "505bf818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = testset[0]\n",
    "weights = torch.load('best_weights.pt')\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    #we are adding an index aloingside channel, height and width\n",
    "    logits = model(image.unsqueeze(0))\n",
    "    #applying softmax activation function to the raw outputs\n",
    "    ps = torch.nn.Softmax(dim =1)(logits)[0] #probabilities\n",
    "    utils.view_classify(image, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8297b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
